{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import random\n",
    "random.seed(5)\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 20NG\n",
    "origin: SetFit/20_newsgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration SetFit--20_newsgroups-bba9acf94c3d61ec\n",
      "Reusing dataset json (/home/v-biyangguo/.cache/huggingface/datasets/SetFit___json/SetFit--20_newsgroups-bba9acf94c3d61ec/0.0.0/a3e658c4731e59120d44081ac10bf85dc7e1388126b92338344ce9661907f253)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "606a86233a3d4d01b2da89e6c46f781f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label', 'label_text'],\n",
       "        num_rows: 11314\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label', 'label_text'],\n",
       "        num_rows: 7532\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset('SetFit/20_newsgroups')\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/v-biyangguo/.cache/huggingface/datasets/SetFit___json/SetFit--20_newsgroups-bba9acf94c3d61ec/0.0.0/a3e658c4731e59120d44081ac10bf85dc7e1388126b92338344ce9661907f253/cache-0d420d30af8f0d62.arrow\n",
      "Loading cached processed dataset at /home/v-biyangguo/.cache/huggingface/datasets/SetFit___json/SetFit--20_newsgroups-bba9acf94c3d61ec/0.0.0/a3e658c4731e59120d44081ac10bf85dc7e1388126b92338344ce9661907f253/cache-533f82bed5639e9a.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text', 'label', 'label_text'],\n",
      "    num_rows: 11096\n",
      "})\n",
      "Dataset({\n",
      "    features: ['text', 'label', 'label_text'],\n",
      "    num_rows: 7370\n",
      "})\n",
      "7767 7767\n",
      "3329 3329\n",
      "7370 7370\n"
     ]
    }
   ],
   "source": [
    "# filter empty contents\n",
    "orig_train_dataset = dataset['train'].filter(lambda x:x['text'] not in ['',None,float('nan')])\n",
    "print(orig_train_dataset)\n",
    "orig_test_dataset = dataset['test'].filter(lambda x:x['text'] not in ['',None,float('nan')])\n",
    "print(orig_test_dataset)\n",
    "\n",
    "\n",
    "orig_train_contents = orig_train_dataset['text']\n",
    "orig_train_labels = orig_train_dataset['label']\n",
    "test_contents = orig_test_dataset['text']\n",
    "test_labels = orig_test_dataset['label']\n",
    "\n",
    "train_contents, dev_contents, train_labels, dev_labels = train_test_split(\n",
    "    orig_train_contents, orig_train_labels, test_size=0.3, random_state=42)\n",
    "\n",
    "print(len(train_contents),len(train_labels))\n",
    "print(len(dev_contents),len(dev_labels))\n",
    "print(len(test_contents),len(test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# full dataset\n",
    "\n",
    "data_path = './20ng_full'\n",
    "if not os.path.exists(data_path):\n",
    "    os.mkdir(data_path)\n",
    "pd.DataFrame({'content':train_contents, 'label':train_labels}).to_csv(f\"{data_path}/train.csv\")\n",
    "pd.DataFrame({'content':dev_contents, 'label':dev_labels}).to_csv(f\"{data_path}/dev.csv\")\n",
    "pd.DataFrame({'content':test_contents, 'label':test_labels}).to_csv(f\"{data_path}/test.csv\")\n",
    "\n",
    "# sub dataset\n",
    "\n",
    "for num in [50,100,200,500,1000]:\n",
    "    data_path = './20ng_%s'%num\n",
    "    if not os.path.exists(data_path):\n",
    "        os.mkdir(data_path)\n",
    "    pd.DataFrame({'content':train_contents[:num], 'label':train_labels[:num]}).to_csv(f\"{data_path}/train.csv\")\n",
    "    pd.DataFrame({'content':dev_contents[:num], 'label':dev_labels[:num]}).to_csv(f\"{data_path}/dev.csv\")\n",
    "    pd.DataFrame({'content':test_contents, 'label':test_labels}).to_csv(f\"{data_path}/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 20)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check\n",
    "data_train = pd.read_csv('20ng_100/train.csv')\n",
    "data_dev = pd.read_csv('20ng_100/dev.csv')\n",
    "len(data_train['label'].value_counts()),len(data_dev['label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Yahoo Answers\n",
    "origin: yahoo_answers_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset yahoo_answers_topics (/home/v-biyangguo/.cache/huggingface/datasets/yahoo_answers_topics/yahoo_answers_topics/1.0.0/0edb353eefe79d9245d7bd7cac5ae6af19530439da520d6dde1c206ee38f4439)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9351c2380c92407d92f416ab88cbaa0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'topic', 'question_title', 'question_content', 'best_answer'],\n",
       "        num_rows: 1400000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'topic', 'question_title', 'question_content', 'best_answer'],\n",
       "        num_rows: 60000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset('yahoo_answers_topics')\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/v-biyangguo/.cache/huggingface/datasets/yahoo_answers_topics/yahoo_answers_topics/1.0.0/0edb353eefe79d9245d7bd7cac5ae6af19530439da520d6dde1c206ee38f4439/cache-e3ea7e3b492b45e6.arrow\n",
      "Loading cached processed dataset at /home/v-biyangguo/.cache/huggingface/datasets/yahoo_answers_topics/yahoo_answers_topics/1.0.0/0edb353eefe79d9245d7bd7cac5ae6af19530439da520d6dde1c206ee38f4439/cache-955613c61170deca.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['id', 'topic', 'question_title', 'question_content', 'best_answer'],\n",
      "    num_rows: 10000\n",
      "})\n",
      "Dataset({\n",
      "    features: ['id', 'topic', 'question_title', 'question_content', 'best_answer'],\n",
      "    num_rows: 9992\n",
      "})\n",
      "7000 7000\n",
      "3000 3000\n",
      "9992 9992\n"
     ]
    }
   ],
   "source": [
    "# filter empty contents\n",
    "# yahoo is too large, so only sample 10k training samples\n",
    "orig_train_dataset = dataset['train'].select(range(10000)).filter(lambda x:x['best_answer'] not in ['',None,float('nan')])\n",
    "print(orig_train_dataset)\n",
    "orig_test_dataset = dataset['test'].select(range(10000)).filter(lambda x:x['best_answer'] not in ['',None,float('nan')])\n",
    "print(orig_test_dataset)\n",
    "\n",
    "# concat question title and answer as the content\n",
    "orig_train_questions = orig_train_dataset['question_title']\n",
    "orig_train_answers = orig_train_dataset['best_answer']\n",
    "orig_train_contents = [' '.join([q,a]) for q,a in zip(orig_train_questions, orig_train_answers)]\n",
    "orig_train_labels = orig_train_dataset['topic']\n",
    "\n",
    "test_questions = orig_test_dataset['question_title']\n",
    "test_answers = orig_test_dataset['best_answer']\n",
    "test_contents = [' '.join([q,a]) for q,a in zip(test_questions, test_answers)]\n",
    "test_labels = orig_test_dataset['topic']\n",
    "\n",
    "train_contents, dev_contents, train_labels, dev_labels = train_test_split(\n",
    "    orig_train_contents, orig_train_labels, test_size=0.3, random_state=42)\n",
    "\n",
    "print(len(train_contents),len(train_labels))\n",
    "print(len(dev_contents),len(dev_labels))\n",
    "print(len(test_contents),len(test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# full dataset\n",
    "data_path = './yahoo10k_full'\n",
    "if not os.path.exists(data_path):\n",
    "    os.mkdir(data_path)\n",
    "pd.DataFrame({'content':train_contents, 'label':train_labels}).to_csv(f\"{data_path}/train.csv\")\n",
    "pd.DataFrame({'content':dev_contents, 'label':dev_labels}).to_csv(f\"{data_path}/dev.csv\")\n",
    "pd.DataFrame({'content':test_contents, 'label':test_labels}).to_csv(f\"{data_path}/test.csv\")\n",
    "\n",
    "# sub dataset\n",
    "for num in [50,100,200,500,1000]:\n",
    "    data_path = './yahoo10k_%s'%num\n",
    "    if not os.path.exists(data_path):\n",
    "        os.mkdir(data_path)\n",
    "    pd.DataFrame({'content':train_contents[:num], 'label':train_labels[:num]}).to_csv(f\"{data_path}/train.csv\")\n",
    "    pd.DataFrame({'content':dev_contents[:num], 'label':dev_labels[:num]}).to_csv(f\"{data_path}/dev.csv\")\n",
    "    pd.DataFrame({'content':test_contents, 'label':test_labels}).to_csv(f\"{data_path}/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 10)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check\n",
    "data_train = pd.read_csv('yahoo10k_50/train.csv')\n",
    "data_dev = pd.read_csv('yahoo10k_50/dev.csv')\n",
    "len(data_train['label'].value_counts()),len(data_dev['label'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HuffPost\n",
    "origin: khalidalt/HuffPost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No config specified, defaulting to: huff_post/default\n",
      "Reusing dataset huff_post (/home/v-biyangguo/.cache/huggingface/datasets/khalidalt___huff_post/default/1.1.0/7e696fa9c5f0fda2ed35e66f7b84cdbb17d017a09c3c05b4e6e864d2a1000499)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f00a431c8c84d5a9afaf55e2e929d99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    test: Dataset({\n",
       "        features: ['category', 'headline', 'authors', 'link', 'short_description', 'date', 'label'],\n",
       "        num_rows: 200853\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset('khalidalt/HuffPost')\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['category', 'headline', 'authors', 'link', 'short_description', 'date', 'label'],\n",
      "    num_rows: 54427\n",
      "})\n",
      "Dataset({\n",
      "    features: ['category', 'headline', 'authors', 'link', 'short_description', 'date', 'label'],\n",
      "    num_rows: 7000\n",
      "})\n",
      "Dataset({\n",
      "    features: ['category', 'headline', 'authors', 'link', 'short_description', 'date', 'label'],\n",
      "    num_rows: 3000\n",
      "})\n",
      "Dataset({\n",
      "    features: ['category', 'headline', 'authors', 'link', 'short_description', 'date', 'label'],\n",
      "    num_rows: 10000\n",
      "})\n",
      "7000 7000 5\n",
      "3000 3000 5\n",
      "10000 10000 5\n"
     ]
    }
   ],
   "source": [
    "# original dataset has 41 classes! too much ! so we only keep 5 classes, same as BBC news\n",
    "selected_categories = ['ENTERTAINMENT','SPORTS','BUSINESS','TECH','POLITICS']\n",
    "categories_translation = {\n",
    "    'ENTERTAINMENT':'entertainment',\n",
    "    'SPORTS':'sport',\n",
    "    'BUSINESS':'business',\n",
    "    'TECH':'tech',\n",
    "    'POLITICS':'politics',\n",
    "} # to be consistent with BBC news dataset\n",
    "# dataset = dataset['test'].filter(lambda x:x['category'] in selected_categories).filter(lambda x:x['short_description'] not in ['',None,float('nan')])\n",
    "print(dataset) # 54427\n",
    "\n",
    "# filter empty contents\n",
    "# huffpost is too large, so only sample 10k training samples\n",
    "alist = list(range(54427))\n",
    "random.shuffle(alist)\n",
    "alist_train = alist[:7000]\n",
    "alist_dev = alist[7000:10000]\n",
    "alist_test = alist[10000:20000]\n",
    "\n",
    "train_dataset = dataset.select(alist_train)\n",
    "dev_dataset = dataset.select(alist_dev)\n",
    "test_dataset = dataset.select(alist_test)\n",
    "print(train_dataset)\n",
    "print(dev_dataset)\n",
    "print(test_dataset)\n",
    "\n",
    "\n",
    "# concat headline and short_description as the content\n",
    "train_questions = train_dataset['headline']\n",
    "train_answers = train_dataset['short_description']\n",
    "train_contents = [' '.join([q,a]) for q,a in zip(train_questions, train_answers)]\n",
    "train_labels = [categories_translation[l] for l in train_dataset['category']]\n",
    "\n",
    "dev_questions = dev_dataset['headline']\n",
    "dev_answers = dev_dataset['short_description']\n",
    "dev_contents = [' '.join([q,a]) for q,a in zip(dev_questions, dev_answers)]\n",
    "dev_labels =  [categories_translation[l] for l in dev_dataset['category']]\n",
    "\n",
    "test_questions = test_dataset['headline']\n",
    "test_answers = test_dataset['short_description']\n",
    "test_contents = [' '.join([q,a]) for q,a in zip(test_questions, test_answers)]\n",
    "test_labels =  [categories_translation[l] for l in test_dataset['category']]\n",
    "\n",
    "print(len(train_contents),len(train_labels),len(set(train_labels)))\n",
    "print(len(dev_contents),len(dev_labels),len(set(dev_labels)))\n",
    "print(len(test_contents),len(test_labels),len(set(test_labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# full dataset\n",
    "data_path = './5huffpost_full'\n",
    "if not os.path.exists(data_path):\n",
    "    os.mkdir(data_path)\n",
    "pd.DataFrame({'content':train_contents, 'label':train_labels}).to_csv(f\"{data_path}/train.csv\")\n",
    "pd.DataFrame({'content':dev_contents, 'label':dev_labels}).to_csv(f\"{data_path}/dev.csv\")\n",
    "pd.DataFrame({'content':test_contents, 'label':test_labels}).to_csv(f\"{data_path}/test.csv\")\n",
    "\n",
    "# sub dataset\n",
    "for num in [50,100,200,500,1000]:\n",
    "    data_path = './5huffpost_%s'%num\n",
    "    if not os.path.exists(data_path):\n",
    "        os.mkdir(data_path)\n",
    "    pd.DataFrame({'content':train_contents[:num], 'label':train_labels[:num]}).to_csv(f\"{data_path}/train.csv\")\n",
    "    pd.DataFrame({'content':dev_contents[:num], 'label':dev_labels[:num]}).to_csv(f\"{data_path}/dev.csv\")\n",
    "    pd.DataFrame({'content':test_contents, 'label':test_labels}).to_csv(f\"{data_path}/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 4)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check\n",
    "data_train = pd.read_csv('5huffpost_50/train.csv')\n",
    "data_dev = pd.read_csv('5huffpost_50/dev.csv')\n",
    "len(data_train['label'].value_counts()),len(data_dev['label'].value_counts()) # make sure the train set contain all categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BBC news\n",
    "origin: SetFit/bbc-news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration SetFit--bbc-news-003ad310d9aedc64\n",
      "Reusing dataset json (/home/v-biyangguo/.cache/huggingface/datasets/SetFit___json/SetFit--bbc-news-003ad310d9aedc64/0.0.0/a3e658c4731e59120d44081ac10bf85dc7e1388126b92338344ce9661907f253)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cd89394a20f430c953565cf0289bc72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label', 'label_text'],\n",
       "        num_rows: 1225\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label', 'label_text'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset('SetFit/bbc-news')\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f628d6ca725a461f931e2b38de8d8edb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text', 'label', 'label_text'],\n",
      "    num_rows: 1225\n",
      "})\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66a89e62eff042a0ad46f2d950b5c5b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text', 'label', 'label_text'],\n",
      "    num_rows: 1000\n",
      "})\n",
      "857 857\n",
      "368 368\n",
      "1000 1000\n"
     ]
    }
   ],
   "source": [
    "# filter empty contents\n",
    "orig_train_dataset = dataset['train'].filter(lambda x:x['text'] not in ['',None,float('nan')])\n",
    "print(orig_train_dataset)\n",
    "orig_test_dataset = dataset['test'].filter(lambda x:x['text'] not in ['',None,float('nan')])\n",
    "print(orig_test_dataset)\n",
    "\n",
    "\n",
    "orig_train_contents = orig_train_dataset['text']\n",
    "orig_train_labels = orig_train_dataset['label_text']\n",
    "test_contents = orig_test_dataset['text']\n",
    "test_labels = orig_test_dataset['label_text']\n",
    "\n",
    "train_contents, dev_contents, train_labels, dev_labels = train_test_split(\n",
    "    orig_train_contents, orig_train_labels, test_size=0.3, random_state=42)\n",
    "\n",
    "print(len(train_contents),len(train_labels))\n",
    "print(len(dev_contents),len(dev_labels))\n",
    "print(len(test_contents),len(test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# full dataset\n",
    "data_path = './bbc_full'\n",
    "if not os.path.exists(data_path):\n",
    "    os.mkdir(data_path)\n",
    "pd.DataFrame({'content':train_contents, 'label':train_labels}).to_csv(f\"{data_path}/train.csv\")\n",
    "pd.DataFrame({'content':dev_contents, 'label':dev_labels}).to_csv(f\"{data_path}/dev.csv\")\n",
    "pd.DataFrame({'content':test_contents, 'label':test_labels}).to_csv(f\"{data_path}/test.csv\")\n",
    "\n",
    "# sub dataset\n",
    "for num in [50,100,200,500,1000]:\n",
    "    data_path = './bbc_%s'%num\n",
    "    if not os.path.exists(data_path):\n",
    "        os.mkdir(data_path)\n",
    "    pd.DataFrame({'content':train_contents[:num], 'label':train_labels[:num]}).to_csv(f\"{data_path}/train.csv\")\n",
    "    pd.DataFrame({'content':dev_contents[:num], 'label':dev_labels[:num]}).to_csv(f\"{data_path}/dev.csv\")\n",
    "    pd.DataFrame({'content':test_contents, 'label':test_labels}).to_csv(f\"{data_path}/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 5)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check\n",
    "data_train = pd.read_csv('bbc_50/train.csv')\n",
    "data_dev = pd.read_csv('bbc_50/dev.csv')\n",
    "len(data_train['label'].value_counts()),len(data_dev['label'].value_counts()) # make sure the train set contain all categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMDB\n",
    "origin: imdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset imdb (/home/v-biyangguo/.cache/huggingface/datasets/imdb/plain_text/1.0.0/2fdd8b9bcadd6e7055e742a706876ba43f19faee861df134affd7a3f60fc38a1)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ebfe07e17b94d4da040f7e57059d7b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    unsupervised: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 50000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset('imdb')\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef4b1bb31cbe47e59c91a9d58ddb3d98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text', 'label'],\n",
      "    num_rows: 25000\n",
      "})\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8597d04318d7402facc2846feb3c6ca7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text', 'label'],\n",
      "    num_rows: 25000\n",
      "})\n",
      "17500 17500\n",
      "7500 7500\n",
      "25000 25000\n"
     ]
    }
   ],
   "source": [
    "# filter empty contents\n",
    "orig_train_dataset = dataset['train'].filter(lambda x:x['text'] not in ['',None,float('nan')])\n",
    "print(orig_train_dataset)\n",
    "orig_test_dataset = dataset['test'].filter(lambda x:x['text'] not in ['',None,float('nan')])\n",
    "print(orig_test_dataset)\n",
    "\n",
    "\n",
    "orig_train_contents = orig_train_dataset['text']\n",
    "orig_train_labels = orig_train_dataset['label']\n",
    "test_contents = orig_test_dataset['text']\n",
    "test_labels = orig_test_dataset['label']\n",
    "\n",
    "train_contents, dev_contents, train_labels, dev_labels = train_test_split(\n",
    "    orig_train_contents, orig_train_labels, test_size=0.3, random_state=42)\n",
    "\n",
    "print(len(train_contents),len(train_labels))\n",
    "print(len(dev_contents),len(dev_labels))\n",
    "print(len(test_contents),len(test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# full dataset\n",
    "data_path = './imdb_full'\n",
    "if not os.path.exists(data_path):\n",
    "    os.mkdir(data_path)\n",
    "pd.DataFrame({'content':train_contents, 'label':train_labels}).to_csv(f\"{data_path}/train.csv\")\n",
    "pd.DataFrame({'content':dev_contents, 'label':dev_labels}).to_csv(f\"{data_path}/dev.csv\")\n",
    "pd.DataFrame({'content':test_contents, 'label':test_labels}).to_csv(f\"{data_path}/test.csv\")\n",
    "\n",
    "# sub dataset\n",
    "for num in [50,100,200,500,1000]:\n",
    "    data_path = './imdb_%s'%num\n",
    "    if not os.path.exists(data_path):\n",
    "        os.mkdir(data_path)\n",
    "    pd.DataFrame({'content':train_contents[:num], 'label':train_labels[:num]}).to_csv(f\"{data_path}/train.csv\")\n",
    "    pd.DataFrame({'content':dev_contents[:num], 'label':dev_labels[:num]}).to_csv(f\"{data_path}/dev.csv\")\n",
    "    pd.DataFrame({'content':test_contents, 'label':test_labels}).to_csv(f\"{data_path}/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 2)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check\n",
    "data_train = pd.read_csv('imdb_50/train.csv')\n",
    "data_dev = pd.read_csv('imdb_50/dev.csv')\n",
    "len(data_train['label'].value_counts()),len(data_dev['label'].value_counts()) # make sure the train set contain all categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SST2\n",
    "origin: glue sst2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c6d6b667d3941c098d82fcaeaaf9927",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/7.78k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b7b8380423f4152a19c696169b2703b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading metadata:   0%|          | 0.00/4.47k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset glue/sst2 (download: 7.09 MiB, generated: 4.81 MiB, post-processed: Unknown size, total: 11.90 MiB) to /home/v-biyangguo/.cache/huggingface/datasets/glue/sst2/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33b8ecc118984c93bcdb9830ecf4db79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/7.44M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "930337361908453eb39eee033f9866d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/67349 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed12eb9b5b94420fb6c33304e35fa3e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/872 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a9a9519b7234590854077b35e3d33ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/1821 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset glue downloaded and prepared to /home/v-biyangguo/.cache/huggingface/datasets/glue/sst2/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72b7807dcbce4cd184f216a624970825",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sentence', 'label', 'idx'],\n",
       "        num_rows: 67349\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['sentence', 'label', 'idx'],\n",
       "        num_rows: 872\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['sentence', 'label', 'idx'],\n",
       "        num_rows: 1821\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset('glue','sst2')\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f975493f039747878ac7473d5891bf13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/68 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['sentence', 'label', 'idx'],\n",
      "    num_rows: 67349\n",
      "})\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d84df4fbff846f59501b0e23f6c078d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['sentence', 'label', 'idx'],\n",
      "    num_rows: 872\n",
      "})\n",
      "47144 47144\n",
      "20205 20205\n",
      "872 872\n"
     ]
    }
   ],
   "source": [
    "# since the sst2 doesn't provide the labels for test set\n",
    "# we use the dev set as the test here, and split dev set from the original train set\n",
    "# filter empty contents\n",
    "orig_train_dataset = dataset['train'].filter(lambda x:x['sentence'] not in ['',None,float('nan')])\n",
    "print(orig_train_dataset)\n",
    "orig_test_dataset = dataset['validation'].filter(lambda x:x['sentence'] not in ['',None,float('nan')])\n",
    "print(orig_test_dataset)\n",
    "\n",
    "\n",
    "orig_train_contents = orig_train_dataset['sentence']\n",
    "orig_train_labels = orig_train_dataset['label']\n",
    "test_contents = orig_test_dataset['sentence']\n",
    "test_labels = orig_test_dataset['label']\n",
    "\n",
    "train_contents, dev_contents, train_labels, dev_labels = train_test_split(\n",
    "    orig_train_contents, orig_train_labels, test_size=0.3, random_state=42)\n",
    "\n",
    "print(len(train_contents),len(train_labels))\n",
    "print(len(dev_contents),len(dev_labels))\n",
    "print(len(test_contents),len(test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# full dataset\n",
    "data_path = './sst2_full'\n",
    "if not os.path.exists(data_path):\n",
    "    os.mkdir(data_path)\n",
    "pd.DataFrame({'content':train_contents, 'label':train_labels}).to_csv(f\"{data_path}/train.csv\")\n",
    "pd.DataFrame({'content':dev_contents, 'label':dev_labels}).to_csv(f\"{data_path}/dev.csv\")\n",
    "pd.DataFrame({'content':test_contents, 'label':test_labels}).to_csv(f\"{data_path}/test.csv\")\n",
    "\n",
    "# sub dataset\n",
    "for num in [50,100,200,500,1000]:\n",
    "    data_path = './sst2_%s'%num\n",
    "    if not os.path.exists(data_path):\n",
    "        os.mkdir(data_path)\n",
    "    pd.DataFrame({'content':train_contents[:num], 'label':train_labels[:num]}).to_csv(f\"{data_path}/train.csv\")\n",
    "    pd.DataFrame({'content':dev_contents[:num], 'label':dev_labels[:num]}).to_csv(f\"{data_path}/dev.csv\")\n",
    "    pd.DataFrame({'content':test_contents, 'label':test_labels}).to_csv(f\"{data_path}/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 2)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check\n",
    "data_train = pd.read_csv('sst2_50/train.csv')\n",
    "data_dev = pd.read_csv('sst2_50/dev.csv')\n",
    "len(data_train['label'].value_counts()),len(data_dev['label'].value_counts()) # make sure the train set contain all categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SST2-longer version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset glue (/home/v-biyangguo/.cache/huggingface/datasets/glue/sst2/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "394f7b359d2b4e10a8bf9ae6ed7cd412",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sentence', 'label', 'idx'],\n",
       "        num_rows: 67349\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['sentence', 'label', 'idx'],\n",
       "        num_rows: 872\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['sentence', 'label', 'idx'],\n",
       "        num_rows: 1821\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset('glue','sst2')\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e3b928cdbe141f2b9f507f259b2f25c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/68 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/v-biyangguo/.cache/huggingface/datasets/glue/sst2/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-7d03d9a4ffc90207.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['sentence', 'label', 'idx'],\n",
      "    num_rows: 8294\n",
      "})\n",
      "Dataset({\n",
      "    features: ['sentence', 'label', 'idx'],\n",
      "    num_rows: 872\n",
      "})\n",
      "5805 5805\n",
      "2489 2489\n",
      "872 872\n"
     ]
    }
   ],
   "source": [
    "# since the sst2 doesn't provide the labels for test set\n",
    "# we use the dev set as the test here, and split dev set from the original train set\n",
    "# filter empty contents\n",
    "orig_train_dataset = dataset['train'].filter(lambda x:len(x['sentence'].split(' ')) > 20)\n",
    "print(orig_train_dataset)\n",
    "orig_test_dataset = dataset['validation'].filter(lambda x:x['sentence'] not in ['',None,float('nan')])\n",
    "print(orig_test_dataset)\n",
    "\n",
    "\n",
    "orig_train_contents = orig_train_dataset['sentence']\n",
    "orig_train_labels = orig_train_dataset['label']\n",
    "test_contents = orig_test_dataset['sentence']\n",
    "test_labels = orig_test_dataset['label']\n",
    "\n",
    "train_contents, dev_contents, train_labels, dev_labels = train_test_split(\n",
    "    orig_train_contents, orig_train_labels, test_size=0.3, random_state=42)\n",
    "\n",
    "print(len(train_contents),len(train_labels))\n",
    "print(len(dev_contents),len(dev_labels))\n",
    "print(len(test_contents),len(test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# full dataset\n",
    "data_path = './sst2-l_full'\n",
    "if not os.path.exists(data_path):\n",
    "    os.mkdir(data_path)\n",
    "pd.DataFrame({'content':train_contents, 'label':train_labels}).to_csv(f\"{data_path}/train.csv\")\n",
    "pd.DataFrame({'content':dev_contents, 'label':dev_labels}).to_csv(f\"{data_path}/dev.csv\")\n",
    "pd.DataFrame({'content':test_contents, 'label':test_labels}).to_csv(f\"{data_path}/test.csv\")\n",
    "\n",
    "# sub dataset\n",
    "for num in [50,100,200,500,1000]:\n",
    "    data_path = './sst2-l_%s'%num\n",
    "    if not os.path.exists(data_path):\n",
    "        os.mkdir(data_path)\n",
    "    pd.DataFrame({'content':train_contents[:num], 'label':train_labels[:num]}).to_csv(f\"{data_path}/train.csv\")\n",
    "    pd.DataFrame({'content':dev_contents[:num], 'label':dev_labels[:num]}).to_csv(f\"{data_path}/dev.csv\")\n",
    "    pd.DataFrame({'content':test_contents, 'label':test_labels}).to_csv(f\"{data_path}/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 2)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check\n",
    "data_train = pd.read_csv('sst2-l_50/train.csv')\n",
    "data_dev = pd.read_csv('sst2-l_50/dev.csv')\n",
    "len(data_train['label'].value_counts()),len(data_dev['label'].value_counts()) # make sure the train set contain all categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Yahoo Answers Clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset yahoo_answers_topics (/home/v-biyangguo/.cache/huggingface/datasets/yahoo_answers_topics/yahoo_answers_topics/1.0.0/0edb353eefe79d9245d7bd7cac5ae6af19530439da520d6dde1c206ee38f4439)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73546dd9bd3344799ae1735ec27e42a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'topic', 'question_title', 'question_content', 'best_answer'],\n",
       "        num_rows: 1400000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'topic', 'question_title', 'question_content', 'best_answer'],\n",
       "        num_rows: 60000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset('yahoo_answers_topics')\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92d043f16ddd40fda3b43d51cdb85a0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1400 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['id', 'topic', 'question_title', 'question_content', 'best_answer'],\n",
      "    num_rows: 10000\n",
      "})\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fda3f82e59242f9ac0f58d17bf8fb77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/60 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['id', 'topic', 'question_title', 'question_content', 'best_answer'],\n",
      "    num_rows: 10000\n",
      "})\n",
      "7000 7000\n",
      "3000 3000\n",
      "10000 10000\n"
     ]
    }
   ],
   "source": [
    "# filter empty contents\n",
    "# yahoo is too large, so only sample 10k training samples\n",
    "orig_train_dataset = dataset['train'].filter(lambda x:len(x['best_answer'])>50).select(range(10000))\n",
    "print(orig_train_dataset)\n",
    "orig_test_dataset = dataset['test'].filter(lambda x:len(x['best_answer'])>50).select(range(10000))\n",
    "print(orig_test_dataset)\n",
    "\n",
    "# concat question title and answer as the content\n",
    "orig_train_contents = orig_train_dataset['best_answer']\n",
    "orig_train_labels = orig_train_dataset['topic']\n",
    "\n",
    "\n",
    "test_contents = orig_test_dataset['best_answer']\n",
    "test_labels = orig_test_dataset['topic']\n",
    "\n",
    "train_contents, dev_contents, train_labels, dev_labels = train_test_split(\n",
    "    orig_train_contents, orig_train_labels, test_size=0.3, random_state=42)\n",
    "\n",
    "print(len(train_contents),len(train_labels))\n",
    "print(len(dev_contents),len(dev_labels))\n",
    "print(len(test_contents),len(test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# full dataset\n",
    "data_path = './yahooA10k_full'\n",
    "if not os.path.exists(data_path):\n",
    "    os.mkdir(data_path)\n",
    "pd.DataFrame({'content':train_contents, 'label':train_labels}).to_csv(f\"{data_path}/train.csv\")\n",
    "pd.DataFrame({'content':dev_contents, 'label':dev_labels}).to_csv(f\"{data_path}/dev.csv\")\n",
    "pd.DataFrame({'content':test_contents, 'label':test_labels}).to_csv(f\"{data_path}/test.csv\")\n",
    "\n",
    "# sub dataset\n",
    "for num in [50,100,200,500,1000]:\n",
    "    data_path = './yahooA10k_%s'%num\n",
    "    if not os.path.exists(data_path):\n",
    "        os.mkdir(data_path)\n",
    "    pd.DataFrame({'content':train_contents[:num], 'label':train_labels[:num]}).to_csv(f\"{data_path}/train.csv\")\n",
    "    pd.DataFrame({'content':dev_contents[:num], 'label':dev_labels[:num]}).to_csv(f\"{data_path}/dev.csv\")\n",
    "    pd.DataFrame({'content':test_contents, 'label':test_labels}).to_csv(f\"{data_path}/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 10)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check\n",
    "data_train = pd.read_csv('yahooA10k_50/train.csv')\n",
    "data_dev = pd.read_csv('yahooA10k_50/dev.csv')\n",
    "len(data_train['label'].value_counts()),len(data_dev['label'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reddit-topics\n",
    "jamescalam/reddit-topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sub', 'title', 'selftext', 'upvote_ratio', 'id', 'created_utc'],\n",
       "        num_rows: 3791\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "data = load_dataset('jamescalam/reddit-topics')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'LanguageTechnology', 'Python', 'investing', 'pytorch'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(data['train']['sub'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "98b0a9b7b4eaaa670588a142fd0a9b87eaafe866f1db4228be72b4211d12040f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.3 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}