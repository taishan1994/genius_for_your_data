{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset('khalidalt/HuffPost')\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_categories = ['ENTERTAINMENT','SPORTS','BUSINESS','TECH','POLITICS']\n",
    "categories_translation = {\n",
    "    'ENTERTAINMENT':'entertainment',\n",
    "    'SPORTS':'sport',\n",
    "    'BUSINESS':'business',\n",
    "    'TECH':'tech',\n",
    "    'POLITICS':'politics',\n",
    "} # to be consistent with BBC news dataset\n",
    "dataset = dataset['test'].filter(lambda x:x['category'] in selected_categories).filter(lambda x:x['short_description'] not in ['',None,float('nan')])\n",
    "print(dataset) # 54427"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "alist = list(range(54427))\n",
    "random.shuffle(alist)\n",
    "alist_train = alist[:50000]\n",
    "alist_dev = alist[50000:54427]\n",
    "alist_test = alist[50000:54427]\n",
    "\n",
    "train_dataset = dataset.select(alist_train)\n",
    "dev_dataset = dataset.select(alist_dev)\n",
    "test_dataset = dataset.select(alist_test)\n",
    "print(train_dataset)\n",
    "print(dev_dataset)\n",
    "print(test_dataset)\n",
    "\n",
    "\n",
    "# concat headline and short_description as the content\n",
    "train_questions = train_dataset['headline']\n",
    "train_answers = train_dataset['short_description']\n",
    "train_contents = [' '.join([q,a]) for q,a in zip(train_questions, train_answers)]\n",
    "train_labels = [categories_translation[l] for l in train_dataset['category']]\n",
    "\n",
    "dev_questions = dev_dataset['headline']\n",
    "dev_answers = dev_dataset['short_description']\n",
    "dev_contents = [' '.join([q,a]) for q,a in zip(dev_questions, dev_answers)]\n",
    "dev_labels =  [categories_translation[l] for l in dev_dataset['category']]\n",
    "\n",
    "test_questions = test_dataset['headline']\n",
    "test_answers = test_dataset['short_description']\n",
    "test_contents = [' '.join([q,a]) for q,a in zip(test_questions, test_answers)]\n",
    "test_labels =  [categories_translation[l] for l in test_dataset['category']]\n",
    "\n",
    "print(len(train_contents),len(train_labels),len(set(train_labels)))\n",
    "print(len(dev_contents),len(dev_labels),len(set(dev_labels)))\n",
    "print(len(test_contents),len(test_labels),len(set(test_labels)))\n",
    "\n",
    "import ossaudiodev\n",
    "data_path = 'data_clf/5huffpost_scorer'\n",
    "if not os.path.exists(data_path):\n",
    "    os.mkdir(data_path)\n",
    "pd.DataFrame({'content':train_contents, 'label':train_labels}).to_csv(f\"{data_path}/train.csv\")\n",
    "pd.DataFrame({'content':dev_contents, 'label':dev_labels}).to_csv(f\"{data_path}/dev.csv\")\n",
    "pd.DataFrame({'content':test_contents, 'label':test_labels}).to_csv(f\"{data_path}/test.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the sketches in the 5huffpost_1000\n",
    "import pandas as pd\n",
    "data = pd.read_csv('data_clf/5huffpost_1000/sega_promptTrue_asonly_False_sega-old_aug4.csv')\n",
    "orig_contents = list(data['content'])[:1000]\n",
    "sega_contents = list(data['content'])[1000:2000]\n",
    "sketches = list(data['sketch'])[1000:2000]\n",
    "labels = list(data['label'])[1000:2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = []\n",
    "for s,l in zip(sketches, labels):\n",
    "    ss.append(s.replace('%s: '%l, ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bart_data = pd.read_csv('data_clf/5huffpost_1000/sega_promptTrue_asonly_False_poor-bart_aug4.csv')\n",
    "bart_contents = list(bart_data['content'])[1000:2000]\n",
    "bart_sketches = list(bart_data['sketch'])[1000:2000]\n",
    "bart_labels = list(bart_data['label'])[1000:2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 1\n",
    "sketches[i], bart_sketches[i], sega_contents[i], bart_contents[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save bart/sega outputs'\n",
    "\n",
    "pd.DataFrame({'orig_text':orig_contents,'gen_text':sega_contents,'sketch':ss,'label':labels})\\\n",
    ".to_csv('nlg_eval/sega_huff.csv')\n",
    "\n",
    "\n",
    "pd.DataFrame({'orig_text':orig_contents,'gen_text':bart_contents,'sketch':ss,'label':labels})\\\n",
    ".to_csv('nlg_eval/bart_huff.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bart = pipeline('text2text-generation',model='facebook/bart-large',device=7)\n",
    "bart('sport: <mask> offseason is smoking <mask> NFL players <mask>', num_beams=3, do_sample=True,max_length=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scorer:\n",
    "'saved_models/5huffpost_scorer_distilbert-base-cased_train.pkl'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# generating for others, with pipeline\n",
    "- t5-commongen\n",
    "- bart-k2t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k2t_ss = []\n",
    "for s in ss:\n",
    "    k2t_ss.append(s.replace(' <mask> ',' ').replace('<mask> ','').replace(' <mask>',''))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 顺便把CBART的输入给弄了\n",
    "with open('nlg_eval/nkeywords.txt','w') as f:\n",
    "    for i,s in enumerate(k2t_ss):\n",
    "        print(f'{i}',file=f)\n",
    "        print(f'Keywords:\t{s}',file=f)\n",
    "        print(f'Ground truth:\t just for test',file=f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbart_gen_contents = []\n",
    "with open('other_gen/CBART-master/outputs/cbart-large_one-billion-words_w1.0_max_insert_label1_insert_mode0_encoder_loss_type0_sample_top_k_5_decoder_chain5_nkeywords.txt','r') as f:\n",
    "    lines = f.readlines()\n",
    "    for line in lines:\n",
    "        if 'Generated sentence:\t' in line:\n",
    "            gen_text = line.replace('Generated sentence:\t','').replace('\\n','')\n",
    "            cbart_gen_contents.append(gen_text)\n",
    "print(len(cbart_gen_contents))\n",
    "\n",
    "pd.DataFrame({'orig_text':orig_contents,'gen_text':cbart_gen_contents,'sketch':k2t_ss,'label':labels})\\\n",
    ".to_csv('nlg_eval/cbart_huff.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = pipeline('text2text-generation',model='facebook/bart-base',device=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = '<mask> machine learning <mask> my research interest <mask> data science <mask>'\n",
    "# s = 'machine learning my research interest data science'\n",
    "model(s,max_length=100, do_sample=True, num_beams=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sega_utils import List2Dataset\n",
    "\n",
    "ss_dataset = List2Dataset(k2t_ss)\n",
    "ss_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "gen_contents = []\n",
    "for out in tqdm(model(\n",
    "    ss_dataset, num_beams=3, do_sample=True, \n",
    "    num_return_sequences=1, max_length=200, \n",
    "    batch_size=50, truncation=True)):\n",
    "    generated_text = out[0]['generated_text']\n",
    "    gen_contents.append(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({'orig_text':orig_contents,'gen_text':gen_contents,'sketch':sketches,'label':labels})\\\n",
    ".to_csv('nlg_eval/sega-t4-l_huff.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ROUGE scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_metric\n",
    "rouge_score = load_metric(\"rouge\")\n",
    "bert_score = load_metric(\"bertscore\")\n",
    "ppl_score = load_metric(\"perplexity\", module_type=\"metric\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# eval_data = pd.read_csv('nlg_eval/sega_huff.csv')\n",
    "# eval_data = pd.read_csv('nlg_eval/sega-t1_huff.csv')\n",
    "eval_data = pd.read_csv('nlg_eval/sega-t4_huff.csv')\n",
    "# eval_data = pd.read_csv('nlg_eval/sega-t4-l_huff.csv')\n",
    "\n",
    "# eval_data = pd.read_csv('nlg_eval/bart_huff.csv')\n",
    "# eval_data = pd.read_csv('nlg_eval/t5cg_huff.csv')\n",
    "# eval_data = pd.read_csv('nlg_eval/cbart_huff.csv')\n",
    "# eval_data = pd.read_csv('nlg_eval/bart-k2t_huff.csv')\n",
    "# eval_data = pd.read_csv('nlg_eval/ilm-sent_huff.csv')\n",
    "# eval_data = pd.read_csv('nlg_eval/ilm-ngram_huff.csv')\n",
    "\n",
    "orig_contents = list(eval_data['orig_text'])\n",
    "gen_contents = list(eval_data['gen_text'])\n",
    "# sketches_mask = list(eval_data['sketch'])\n",
    "# sketches = list(eval_data['sketch'])\n",
    "\n",
    "gen_contents = sketches\n",
    "# for i in [5,57,70,89,98]:\n",
    "#     print(i)\n",
    "#     print(orig_contents[i])\n",
    "#     print(sketches[i])\n",
    "#     print(gen_contents[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### ROUGE\n",
    "scores = rouge_score.compute(\n",
    "    predictions=gen_contents, references=orig_contents)\n",
    "for k in scores:\n",
    "    print(f'{k}. F1: {scores[k].mid.fmeasure * 100}, Recall:{scores[k].mid.recall * 100}')\n",
    "\n",
    "# Bert-score\n",
    "other_contents = []\n",
    "for s,c in zip(sketches, gen_contents):\n",
    "    for f in s:\n",
    "        c = c.replace(f,'')\n",
    "    other_contents.append(c)\n",
    "results = bert_score.compute(predictions=other_contents, references=sketches, lang=\"en\",model_type='bert-base-uncased')['f1']\n",
    "print('bert-score', sum(results)/len(results))\n",
    "\n",
    "##### ppl\n",
    "scores = ppl_score.compute(input_texts=gen_contents, model_id='gpt2')\n",
    "ppl = scores['mean_perplexity']\n",
    "import math\n",
    "print('ppl',ppl, math.pow(2,math.log(ppl)))\n",
    "\n",
    "##### length ratio\n",
    "lr = [len(gen_c)/len(orig_c) for gen_c,orig_c in zip(gen_contents,orig_contents)]\n",
    "print('length ratio',sum(lr)/len(lr))\n",
    "\n",
    "##### sketch-lost\n",
    "scores = []\n",
    "for s,c in zip(sketches, gen_contents):\n",
    "    items = s.split(' ')\n",
    "    N = len(items)\n",
    "    n = 0\n",
    "    for item in items:\n",
    "        if item not in c:\n",
    "            n += 1\n",
    "    scores.append(n/N)\n",
    "slost = sum(scores)/len(scores)\n",
    "print('sketch-lost:',slost)\n",
    "\n",
    "##### sketch-fragment lost\n",
    "n = 0\n",
    "N = 0\n",
    "for s,c in zip(sketches_mask, gen_contents):\n",
    "    frags = s.split('<mask>')\n",
    "    for f in frags:\n",
    "        N += 1\n",
    "        f = f.strip()\n",
    "        if f not in c:\n",
    "            n += 1\n",
    "flost = n/N\n",
    "print('f-lost:',flost)\n",
    "print('avg-lost:',(slost+flost)/2)\n",
    "\n",
    "\n",
    "##### Novel Mentions\n",
    "from nltk.tokenize import word_tokenize\n",
    "orig_words = []\n",
    "for c in orig_contents:\n",
    "    orig_words += word_tokenize(c)\n",
    "orig_words = list(set(orig_words))\n",
    "\n",
    "gen_words = []\n",
    "for c in gen_contents:\n",
    "    gen_words += word_tokenize(c)\n",
    "gen_words = list(set(gen_words))\n",
    "\n",
    "n = 0\n",
    "for w in gen_words:\n",
    "    if w not in orig_words:\n",
    "        n += 1\n",
    "print('NM:',n, \"NM rate:\",n/len(orig_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sketches_mask[10], gen_contents[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classifier score\n",
    "\n",
    "labels = list(eval_data['label'])\n",
    "unique_labels = sorted(list(set(labels)))\n",
    "label2idx = {unique_labels[i]: i for i in range(len(unique_labels))}\n",
    "idx2label = {label2idx[label]: label for label in label2idx}\n",
    "\n",
    "label2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, tokenizer, texts, labels, label2idx, maxlen):\n",
    "        self.tokenizer = tokenizer\n",
    "        # 我先不用padding，后面通过data_collator来做dynamic padding\n",
    "        texts = [t if (t != None and str(t) != 'nan') else '' for t in texts]\n",
    "        self.encodings = tokenizer(texts, truncation=True, max_length=maxlen)\n",
    "        self.labels = labels\n",
    "        self.label2idx = label2idx\n",
    "    def __getitem__(self, idx):\n",
    "        item = {k:torch.tensor(v[idx]) for k,v in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.label2idx[self.labels[idx]])  # labels字段应该保存label的idx，而不是具体label名\n",
    "        return item\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "def get_dataloader_from_list(texts, labels, tokenizer, label2idx, maxlen, bsz, collate_fn, shuffle=True):\n",
    "    dataset = MyDataset(tokenizer, texts, labels, label2idx, maxlen)\n",
    "    dataloader = DataLoader(dataset, batch_size=bsz, collate_fn=collate_fn, shuffle=shuffle)\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer,DataCollatorWithPadding\n",
    "from tqdm import tqdm\n",
    "\n",
    "clf_checkpoint = 'saved_models/5huffpost_scorer_bert-base-uncased_train.pkl'\n",
    "clf_model = AutoModelForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=len(unique_labels))\n",
    "clf_model.load_state_dict(torch.load(clf_checkpoint)) # the non-aug model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu',1)\n",
    "print('>>> ',device)\n",
    "clf_model.to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "bz = 32\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "train_dataloader = get_dataloader_from_list(\n",
    "    gen_contents, labels, tokenizer, label2idx, 100, bz, \n",
    "    data_collator, shuffle=False) # must set `shuffle=False` to keep the original order\n",
    "\n",
    "\n",
    "clf_model.eval()\n",
    "all_true_label_probs = []\n",
    "i = 0\n",
    "for batch in tqdm(train_dataloader):\n",
    "    batch = {k:v.to(device) for k,v in batch.items()}\n",
    "    logits = clf_model(**batch).logits\n",
    "    probs = torch.softmax(logits, dim=1)\n",
    "    # all_probs.append(probs.cpu())\n",
    "    label_ids = [label2idx[label] for label in labels[i*bz :(i+1)*bz]]\n",
    "    label_ids = torch.LongTensor([[idx] for idx in label_ids])\n",
    "    true_label_probs = probs.gather(1, label_ids.to(device))\n",
    "    # print(true_label_probs.shape)\n",
    "    # print(true_label_probs.view(-1,).tolist())\n",
    "    all_true_label_probs += true_label_probs.view(-1,).tolist()\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1-sum(all_true_label_probs)/len(all_true_label_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_contents[10]"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "98b0a9b7b4eaaa670588a142fd0a9b87eaafe866f1db4228be72b4211d12040f"
  },
  "kernelspec": {
   "display_name": "Python 3.6.10 64-bit ('conda': virtualenv)",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": ""
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}